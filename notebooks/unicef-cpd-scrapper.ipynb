{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function loaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re \n",
    "from collections import namedtuple\n",
    "from PyPDF2 import PdfReader\n",
    "from deltalake.writer import write_deltalake\n",
    "from tqdm.notebook import tqdm,trange\n",
    "\n",
    "# importing module\n",
    "import logging\n",
    " \n",
    "# Create and configure logger\n",
    "logging.basicConfig(filename=\"../pipeline.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')\n",
    "\n",
    "logger=logging.getLogger()\n",
    "\n",
    "\n",
    "# Setting the threshold of logger to DEBUG\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "today = datetime.today().strftime(\"%d-%b-%Y\")\n",
    "\n",
    "target_urls = {\n",
    "    \"CPD\":\"https://www.unicef.org/executiveboard/country-programme-documents\",\n",
    "    \"SITAN\":\"\",\n",
    "    \"COARS\":\"\"\n",
    "}\n",
    "\n",
    "urls = {tag: [] for tag in target_urls}\n",
    "data = []\n",
    "url_items = []\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "\n",
    "def request(url,stream=True):\n",
    "    \n",
    "    if stream:\n",
    "        \n",
    "        \"\"\"headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            \"Accept-Ranges\": \"bytes\",\n",
    "            \"Content-Length\": \"301821\",\n",
    "            \"Content-Security-Policy\": \"default-src 'self'; frame-src youtube.com www.youtube.com; frame-ancestors 'none';\",\n",
    "            \"Content-Type\": \"application/pdf\",\n",
    "            \"Date\": \"Tue, 07 May 2024 17:11:16 GMT\",\n",
    "            \"Last-Modified\": \"Sat, 08 Jun 2019 02:57:00 GMT\",\n",
    "            \"Server\": \"Microsoft-IIS/10.0\",\n",
    "            \"Strict-Transport-Security\": \"max-age=31536000; includeSubDomains; preload\",\n",
    "            \"X-Content-Type-Options\": \"nosniff\",\n",
    "            \"X-Frame-Options\": \"DENY\",\n",
    "            \"X-Powered-By\": \"ASP.NET\",\n",
    "            \"X-Xss-Protection\": \"1; mode=block\"\n",
    "        }\"\"\"\n",
    "        \n",
    "        response=requests.get(url, headers=headers, stream=True)\n",
    "        logging.info(f\" url {url} {response.status_code} {response.reason} {stream}\")\n",
    "\n",
    "    \n",
    "    else:\n",
    "        response=requests.get(url, headers=headers)\n",
    "        \n",
    "        response=requests.get(url, headers=headers, stream=True)\n",
    "        logging.info(f\" url {url} {response.status_code} {response.reason} {stream}\")\n",
    "\n",
    "     \n",
    "      \n",
    "    return response\n",
    "\n",
    "def sort_language(url):\n",
    "    language_mapping = {\n",
    "            \"AR\": \"AR\",\n",
    "            \"EN\": \"EN\",\n",
    "            \"English\": \"EN\",\n",
    "            \"ENG\":\"EN\",\n",
    "            \"ES\": \"ES\",\n",
    "            \"SP\": \"ES\",\n",
    "            \"Spanish\": \"ES\",\n",
    "            \"FR\": \"FR\",\n",
    "            \"French\": \"FR\",\n",
    "            \"RU\": \"RU\",\n",
    "            \"CH\": \"CH\",\n",
    "            \"final\": \"EN\"\n",
    "        }\n",
    "    \n",
    "    for language_code, language_name in language_mapping.items():\n",
    "            #print(language_code)\n",
    "            if language_code in url:\n",
    "                return language_name\n",
    "            if language_code in url is None:\n",
    "                return \"EN\"\n",
    "\n",
    "\n",
    "def sort_url(url):\n",
    "    lowercase_url = url.lower()\n",
    "\n",
    "    # Check for CPE related URLs\n",
    "    if any(keyword in url for keyword in (\"CEP\", \"CRR\")):\n",
    "        #print(\"CPE: {}\".format(url))\n",
    "        return \"CPE\"\n",
    "\n",
    "    # Check for Extension related URLs\n",
    "    if \"extension\" in lowercase_url:\n",
    "        #print(\"Extension: {}\".format(url))\n",
    "        return \"Extension\"\n",
    "\n",
    "    # Check for SRM related URLs\n",
    "    srm_keywords = (\"srm\", \"summary_results\", \"results_matrx\", \"matrice_de_resultats\", \"_matrix\",\n",
    "                    \"summary_results_matrix\", \"matrix\", \"resultsmatrix\", \"results_matrix\", \"results-matrix\",\"rrf\")\n",
    "    if any(keyword in lowercase_url for keyword in srm_keywords):\n",
    "        #print(\"SRM: {}\".format(url))\n",
    "        return \"SRM\"\n",
    "\n",
    "    # Check for other UN-related URLs\n",
    "    un_keywords = (\"UNSDCF\", \"UNSPF\", \"UNDAF\", \"UNCCSF\", \"UNPAF\")\n",
    "    for keyword in un_keywords:\n",
    "        if keyword in url:\n",
    "            #print(f\"{keyword}: {url}\")\n",
    "            return keyword\n",
    "  \n",
    "    # Check for specific CPD and other cases\n",
    "    if \"Rwanda-UN-2018-2023-2018.04.10\" in url:\n",
    "        #print(\"UNDAP: {}\".format(url))\n",
    "        return \"UNDAP\"\n",
    "\n",
    "    # Check for specific CPD and other cases\n",
    "    if \"Eastern_Caribbean_multicountry_2012-2016_20_Oct_2012\" in url:\n",
    "        #print(\"CPD: {}\".format(url))\n",
    "        return \"CPD\"\n",
    "    \n",
    "    # check for specific SRM \n",
    "    if \"Afghanistan_CPD-SRM\" in url:\n",
    "        #print(\"SRM: {}\".format(url))\n",
    "        return \"SRM\"\n",
    "    \n",
    "    # Check for CPD related URLs\n",
    "    cpd_keywords = (\"cpd\", \"cdp\", \"final_approved\", \"spd\", \"apd\", \"ods\", \"final\")\n",
    "    if any(keyword in lowercase_url for keyword in cpd_keywords):\n",
    "        language = sort_language(url)\n",
    "        #print(f\"CPD {language}: {url}\")\n",
    "        return \"CPD\"\n",
    "\n",
    "    # If none of the above conditions match, print the URL as it is\n",
    "    #print(url)\n",
    "\n",
    "\n",
    "def get_tags_from_url_name(url:{list}) -> list:\n",
    "\n",
    "    # Get the tags from the URL based on the target_urls dictoinary \n",
    "    for url in urls:\n",
    "\n",
    "        # for CPD\n",
    "        if sort_url(url) == \"CPD\":\n",
    "            \n",
    "            # regx to get year-PL29-countryname in CPD\n",
    "            regx = re.compile(r'(^\\d{4})(\\D+\\d+-)([A-Za-z]+)')\n",
    "            metadata = namedtuple(\"metadata\",\"url year doc_code country_name\")\n",
    "            \n",
    "            url = url.split(\"/\")[-1]\n",
    "            search = regx.search(url)\n",
    "            if search:\n",
    "                year = search.group(1)\n",
    "                doc_code = search.group(2).replace(\"-\",\"\")\n",
    "                country_name = search.group(3)\n",
    "                url_items.append(metadata(url,year,doc_code,country_name))\n",
    "    \n",
    "    \n",
    "    return url_items\n",
    "\n",
    "\n",
    "def download_pdf(path,url):\n",
    "    try:\n",
    "        response = request(url,stream=True)\n",
    "        logging.info(f\" url {url} {response.status_code} {response.reason}\")\n",
    "\n",
    "\n",
    "        with open(f\"{path}\", 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def check_pdf(url,path, output_text=False):\n",
    "    # get all text in pdf\n",
    "    pdf_text = []\n",
    "\n",
    "    # Initialize variable to track if any page has text\n",
    "    has_text = False\n",
    "\n",
    "    # check if path exists\n",
    "    path_exsist = os.path.exists(path)\n",
    "\n",
    "    if path_exsist:\n",
    "        logging.info(f\" Path {path} exists\")\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(path)\n",
    "            \n",
    "            # Check each page for text content\n",
    "            for i,page in enumerate(reader.pages):\n",
    "                \n",
    "                if page.extract_text():\n",
    "                    has_text = True\n",
    "\n",
    "                    if output_text:\n",
    "                        pdf_text.append({url},{i:page.extract_text()})\n",
    "                    else:\n",
    "                        None\n",
    "                        \n",
    "        except Exception as e:\n",
    "            has_text = False\n",
    "    else :\n",
    "\n",
    "        has_text = False\n",
    "        pdf_text = None\n",
    "        logging.info(f\" Path {path} does not exists\")\n",
    "\n",
    "    logging.info(f\" Has_text {has_text} {path}\")\n",
    "\n",
    "    return has_text , pdf_text\n",
    "\n",
    "print(\"function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce6b64ae2824b8ea54aac79eb9cf013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.info(f\"Downloading HTMLs from URLS, started at {today} {time.strftime('%H:%M:%S', time.gmtime())} GMT\")\n",
    "\n",
    "for tag in tqdm(target_urls):\n",
    "\n",
    "    filepath = f'../sources/html/{tag}'\n",
    "    \n",
    "    if tag == \"CPD\":\n",
    "        \n",
    "        response = request(target_urls[tag]).text\n",
    "        \n",
    "        soup = BeautifulSoup(response,'lxml')\n",
    "        \n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "        if target_urls[tag] is not None:\n",
    "            # Define the file path\n",
    "            file_path = f'{filepath}/{tag}.html'\n",
    "            \n",
    "            # Open the file in write mode\n",
    "            with open(file_path, 'w') as file:\n",
    "                # Write the prettified soup object to the file\n",
    "                file.write(soup.prettify())\n",
    "            \n",
    "            #print(f\"File saved: {file_path}, os.path.exists(file_path): {os.path.exists(file_path)}\")\n",
    "\n",
    "            logging.info(f\"File saved: {file_path}, os.path.exists(file_path): {os.path.exists(file_path)}\")\n",
    "\n",
    "            for td in soup.find_all('td'):\n",
    "                for a in td.find_all('a', href=True):\n",
    "                    if a[\"href\"].endswith(\".pdf\"):\n",
    "                        href = a['href']\n",
    "                        http = (\"https://\",\"http://\")\n",
    "                        if not any(keyword in href for keyword in http):\n",
    "                            base_url = \"https://www.unicef.org\"\n",
    "                            url= \"{}{}\".format(base_url,href)\n",
    "                            urls[tag].append(url)\n",
    "                        elif not a['href'].startswith((\"https://undocs.org/E/ICEF\",\"http://undocs.org/E/ICEF\", \"https://unsdg.un.org/\")):\n",
    "                            urls[tag].append(a['href'])\n",
    "                            pass\n",
    "\n",
    "df = pd.DataFrame.from_dict(urls,orient='index').transpose()\n",
    "df.head(5)\n",
    "\n",
    "logging.info(f\"Downloaded HTMLs from URLS, completed at {today} {time.strftime('%H:%M:%S', time.gmtime())} GMT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c454e03aafe2430f827491c64583d134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.info(f\"Downloading Files from URLS, start at {today} {time.strftime('%H:%M:%S', time.gmtime())} GMT\")\n",
    "\n",
    "for tag in urls:\n",
    "\n",
    "    if tag == \"CPD\":        \n",
    "        cpd_urls = urls[\"CPD\"]\n",
    "        for url in tqdm(cpd_urls):\n",
    "            doc_type = sort_url(url)\n",
    "            language = sort_language(url)\n",
    "            filename = url.split(\"/\")[-1]\n",
    "            filetype = \"pdf\" if url.split(\"/\")[-1].endswith(\"pdf\") else \"none\"\n",
    "            scrapped_date = \"5-May-2024\"\n",
    "\n",
    "            \n",
    "            if language is None:\n",
    "                filepath = f'../sources/{filetype}/docs/{doc_type}/'\n",
    "            else:\n",
    "                filepath = f\"../sources/{filetype}/docs/{doc_type}/{language}/\"\n",
    "            \n",
    "            os.makedirs(filepath, exist_ok=True)\n",
    "            \n",
    "            path = filepath + filename\n",
    "\n",
    "            has_text, text = check_pdf(url, path, output_text=True)\n",
    "            print( has_text, text)\n",
    "\n",
    "            if not  os.path.exists(filepath + filename) :\n",
    "                pdf_response = request(url, stream=True)\n",
    "                \n",
    "                #print(f\"Downloading New Files from {url}\")\n",
    "                logging.info(f\"Downloading New Files from {url}\")\n",
    "                \n",
    "                download_pdf(path,url)\n",
    "\n",
    "\n",
    "                #print(f\"File: {filename} File Created: {os.path.exists(filepath + filename)} Has Text: {has_text}\")\n",
    "                logging.info(f\"File: {filename} File Created: {os.path.exists(filepath + filename)} Has Text: {has_text}\")\n",
    "                data.append([doc_type,url,filepath + filename,filename,filetype,today,language,has_text,text])\n",
    "                \n",
    "                df = pd.DataFrame(data,columns=(\"doc_type\",\"url\",\"file location\",\"filename\",\"filetype\",\"scrapped-date\",\"language\",\"downloaded\",\"Content\"))\n",
    "                metadata_filepath = \"../META-INF/\"\n",
    "                os.makedirs(metadata_filepath, exist_ok=True)\n",
    "                df.to_csv(f\"{metadata_filepath}executiveboard-cpd-metadata.csv\",index=True)\n",
    "\n",
    "            else:\n",
    "                print(f\"File: {filename} File Already Exist!: {os.path.exists(filepath + filename)}\")\n",
    "                logging.info(f\"File: {filename} File Already Exist!: {os.path.exists(filepath + filename)}\")\n",
    "\n",
    "                data.append([doc_type,url,filepath + filename,filename,filetype,today,language,has_text,text])\n",
    "\n",
    "                df = pd.DataFrame(data,columns=(\"doc_type\",\"url\",\"file location\",\"filename\",\"filetype\",\"scrapped-date\",\"language\",\"downloaded\",\"Content\"))\n",
    "                metadata_filepath = \"../META-INF/\"\n",
    "                os.makedirs(metadata_filepath, exist_ok=True)\n",
    "                df.to_csv(f\"{metadata_filepath}executiveboard-cpd-metadata-test.csv\",index=True)\n",
    "\n",
    "                break\n",
    "\n",
    "#df = pd.read_csv('../META-INF/executiveboard-cpd-metadata-test.csv')\n",
    "#df.value_counts(subset=['downloaded'])\n",
    "\n",
    "logging.info(f\"Downloaded Files from URLS, completed at {today} {time.strftime('%H:%M:%S', time.gmtime())} GMT\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
